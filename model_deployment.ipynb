{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f5d412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/model.py\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def create_vit(num_classes: int=6,\n",
    "               seed: int=42):\n",
    "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "    \n",
    "    transforms = weights.transforms()\n",
    "    \n",
    "    model = torchvision.models.vit_b_16(weights=weights)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    torch.manual_seed(seed)\n",
    "    model.heads = torch.nn.Sequential(torch.nn.LayerNorm(normalized_shape=768),\n",
    "                                      torch.nn.Linear(in_features=768, out_features=num_classes))\n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "976a9262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Copying scenes\\seg_test\\buildings\\20140.jpg to app\\examples\\20140.jpg\n",
      "[INFO] Copying scenes\\seg_test\\forest\\20225.jpg to app\\examples\\20225.jpg\n",
      "[INFO] Copying scenes\\seg_test\\glacier\\20087.jpg to app\\examples\\20087.jpg\n",
      "[INFO] Copying scenes\\seg_test\\mountain\\20176.jpg to app\\examples\\20176.jpg\n",
      "[INFO] Copying scenes\\seg_test\\sea\\20247.jpg to app\\examples\\20247.jpg\n",
      "[INFO] Copying scenes\\seg_test\\street\\20080.jpg to app\\examples\\20080.jpg\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Create an examples directory\n",
    "app = Path(\"app/\")\n",
    "scene_recog_app_path = app / \"examples\"\n",
    "scene_recog_app_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Collect three random test dataset image paths\n",
    "scene_recog_examples = [Path(\"scenes/seg_test/buildings/20140.jpg\"),\n",
    "                            Path(\"scenes/seg_test/forest/20225.jpg\"),\n",
    "                            Path(\"scenes/seg_test/glacier/20087.jpg\"), \n",
    "                            Path(\"scenes/seg_test/mountain/20176.jpg\"),\n",
    "                            Path(\"scenes/seg_test/sea/20247.jpg\"),\n",
    "                            Path(\"scenes/seg_test/street/20080.jpg\")]\n",
    "\n",
    "# 3. Copy the three random images to the examples directory\n",
    "for example in scene_recog_examples:\n",
    "    destination = scene_recog_app_path / example.name\n",
    "    print(f\"[INFO] Copying {example} to {destination}\")\n",
    "    shutil.copy2(src=example, dst=destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc1ef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/app.py\n",
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from model import create_vit\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "class_names = [\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\"]\n",
    "\n",
    "vit_model, vit_transforms = create_vit(num_classes=len(class_names),\n",
    "                                       seed=42)\n",
    "\n",
    "vit_model.load_state_dict(\n",
    "    torch.load(\n",
    "        f=\"pretrained_vit_feature_extractor_scene_recognition.pth\", \n",
    "        map_location=torch.device(\"cpu\")\n",
    "    )\n",
    ")\n",
    "\n",
    "def predict(img):\n",
    "    start_timer = timer()\n",
    "    \n",
    "    img = vit_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    vit_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        pred_prob = torch.softmax(vit_model(img), dim=1)\n",
    "        \n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_prob[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    pred_time = round(timer() - start_timer, 5)\n",
    "    \n",
    "    return pred_labels_and_probs, pred_time\n",
    "\n",
    "title = \"Scene Recognition: Intel Image Classification\"\n",
    "description = \"A ViT feature extractor Computer Vision model to classify images of scenes from 1 out of 6 classes.\"\n",
    "article = \"Access project repository at [GitHub](https://github.com/Ammar2k/intel_image_classification)\"\n",
    "\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "demo = gr.Interface(fn=predict, \n",
    "                    inputs=gr.Image(type=\"pil\"),\n",
    "                    outputs=[gr.Label(num_top_classes=6, label=\"Predictions\"), \n",
    "                    gr.Number(label=\"Prediction time(s)\")],\n",
    "                    examples=example_list,\n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article\n",
    "                   )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1a81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "# import os\n",
    "# import torch\n",
    "\n",
    "# from app.model import create_vit\n",
    "# from timeit import default_timer as timer\n",
    "# from typing import Tuple, Dict\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "# class_names = [\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\"]\n",
    "\n",
    "# vit_model, vit_transforms = create_vit(num_classes=len(class_names),\n",
    "#                                        seed=42)\n",
    "\n",
    "# vit_model.load_state_dict(\n",
    "#     torch.load(\n",
    "#         f=\"app/pretrained_vit_feature_extractor_scene_recognition.pth\", \n",
    "#         map_location=torch.device(\"cpu\")\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# def predict(img):\n",
    "#     start_timer = timer()\n",
    "    \n",
    "#     img = vit_transforms(img).unsqueeze(0)\n",
    "    \n",
    "#     vit_model.eval()\n",
    "#     with torch.inference_mode():\n",
    "#         pred_prob = torch.softmax(vit_model(img), dim=1)\n",
    "        \n",
    "#     pred_labels_and_probs = {class_names[i]: round(pred_prob[0][i].item(), 3) for i in range(len(class_names))}\n",
    "    \n",
    "#     pred_time = round(timer() - start_timer, 5)\n",
    "    \n",
    "#     return pred_labels_and_probs, pred_time\n",
    "\n",
    "# title = \"Scene Recognition: Intel Image Classification\"\n",
    "# description = \"A ViT feature extractor Computer Vision model to classify images of scenes from 1 out of 6 classes.\"\n",
    "# article = \"Access project repository at [GitHub](https://github.com/Ammar2k/intel_image_classification)\"\n",
    "\n",
    "# example_list = [[\"app/examples/\" + example] for example in os.listdir(\"app/examples\")]\n",
    "\n",
    "# demo = gr.Interface(fn=predict, \n",
    "#                     inputs=gr.Image(type=\"pil\"),\n",
    "#                     outputs=[gr.Label(num_top_classes=6, label=\"Predictions\"), \n",
    "#                     gr.Number(label=\"Prediction time(s)\")],\n",
    "#                     examples=example_list,\n",
    "#                     title=title,\n",
    "#                     description=description,\n",
    "#                     article=article\n",
    "#                    )\n",
    "\n",
    "# demo.launch(debug=False, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ba06e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/requirements.txt\n",
    "torch\n",
    "torchvision\n",
    "gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080be64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
